---
title: "Lab 4"
author: "Robert Cervantes"
date: "`r Sys.Date()`"
output: 
  html_document:
    toc: yes
    toc_float: yes
    theme: spacelab
    highlight: pygments
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Tasks

These are the tasks for lab 4

## Task 1

Get Working directory

```{r}
getwd()
```


## Task 2

Read data in and get the last 6 lines of the data set

```{r}
spruce.df = read.csv("SPRUCE.csv")
tail(spruce.df)
```

## Task 3

## Lowess smoother scatter plot

```{r}
library(s20x)
```

```{r}
trendscatter(Height~BHDiameter, data = spruce.df, f = 0.5)
```
## Make a linear model project
```{r}
spruce.lm = with(spruce.df, lm(Height~BHDiameter))
```

## Find residuals
```{r}
height.res = residuals(spruce.lm)
```

## Find fitted values
```{r}
height.fit = fitted(spruce.lm)
```

## Residual vs Fitted Values
```{r}
plot(y = height.res, x = height.fit)
```
## Residuals vs Fitted using Trendscatter
```{r}
trendscatter(y = height.res, x = height.fit)
```
## Shape of the plot
The plot given shows a parabola in the shape of an n. It is more symmetrical compared to the 1st trendscatter plot. The trendscatter plot is a line going up diangly to the right while this 2nd trendscattter plot goes back down about half way.

What this means is the 1st trendscatter is showing trend of the raw data while the 2nd trendscatter is showing model misfit. The n shape is a signal that linear model is not perfectly capturing the true relationship (Meaning a straight line is not the best model for the data)

## Residual Plot
```{r}
plot(spruce.lm, which = 1)
```

## Check Normality
```{r}
normcheck(spruce.lm, shapiro.wilk = TRUE)
```

## P-Value and Null Hypothesis
The p-value is 0.29. Since the value is greater then 0.05 we accept the null hypothesis which means the residuals do not show strong evidence against being normally distributed. 

## Evaluating the model 
```{r}
round(mean(height.res), 4)
```
The mean of the resiuals is 0 which means the residuals are centered around 0.


## Conclusion 

Based on the raw data plot and the residuals vs. fitted values plot, the straight line captures the overall trend reasonably well, but the slight curvature in the residuals suggests the linear model is an approximation rather than a perfect fit.

## Task 4

## Fit Quadratic 
```{r}
quad.lm = lm(Height~BHDiameter + I(BHDiameter ^ 2), data = spruce.df)

summary(quad.lm)
```

## Scatterplot w/ quadratic curve
```{r}
coef(quad.lm)
```
```{r}
plot(spruce.df)

myplot = function(x) {
  quad.lm$coef[1] + quad.lm$coef[2] * x + quad.lm$coef[3] * x ^ 2
}
curve(myplot, lwd = 2, col = "steelblue", add = TRUE)
```

## Make quad fit
```{r}
quad.fit = fitted(quad.lm)
```

## Residuals vs. fitted
```{r}
plot(quad.lm, which = 1)
```

## Check normality 
```{r}
normcheck(quad.lm, shapiro.wilk = TRUE)
```

## Conclusion 
The residuals from the quadratic model appear roughly normally distributed, as indicated by the Shapiro-Wilk p-value of 0.684. This suggests that the quadratic model is reasonable for fitting the Height vs BHDiameter data rather then the linear model.

## Task 5

## Summarize quad.lm
```{r}
summary(quad.lm)
```

## Beta values

\[
\beta_0 = 0.860896, \quad \beta_1 = 1.469592, \quad \beta_2 = -0.027457
\]


## Interval estimates
```{r}
ciReg(quad.lm)
```

## Equation of fitted line
\[
\text{Height} = \beta_0 + \beta_1 \cdot \text{BHDiameter} + \beta_2 \cdot \text{BHDiameter}^2
= 0.860896 + 1.469592 \cdot \text{BHDiameter} - 0.027457 \cdot \text{BHDiameter}^2
\]

## Height Predictions
```{r}
predict(quad.lm, data.frame(BHDiameter = c(15, 18, 20)))
```

## Comparison 
```{r}
predict(spruce.lm, data.frame(BHDiameter = c(15, 18, 20)))
```
The predictions made by quad.lm follow quadratic growth, they are larger in spruce.lm

## Multiple R-Squared
## Quad lm 
```{r}
summary(quad.lm)$r.squared
```
\[
R^2 = 0.7741266
\]

## Spruce.lm
```{r}
summary(spruce.lm)$r.squared
```
\[
R^2 = 0.6569146
\]

## Adjusted R-squared
```{r}
summary(quad.lm)$adj.r.squared
```

```{r}
summary(spruce.lm)$adj.r.squared
```

The R^2 adjusted shows how well the data fits the model. The more variables are added and model improves that means R^2 increases.

## Meaning of Multiple R-squared
The multiple R^2 is a measure of how well the predictor variables explain the variation in the response variable.

a higher R^2 indicates the model fits the data better, and the quadratic model usually has a higher R^2 than the linear model because it can capture the curvature in the data.

## Model with most variability
```{r}
summary(quad.lm)$r.squared
```

```{r}
summary(quad.lm)$adj.r.squared
```

```{r}
summary(spruce.lm)$r.squared
```

```{r}
summary(spruce.lm)$adj.r.squared
```

quad.lm explains the most variability in height because the R^2 values are greater then spruce.lm

## Anova 
```{r}
anova(spruce.lm)
```

```{r}
anova(quad.lm)
```

```{r}
anova(spruce.lm, quad.lm)
```

My conclusion is that quad.lm is a better model for the data. It has smaller RSS value meaning its a better relationship for fitting the data.

## TSS, RSS, MSS
```{r}
height.qfit = fitted(quad.lm)
```

## TSS
```{r}
TSS = with(spruce.df, sum((Height - mean(Height)) ^ 2))
TSS
```

## MSS
```{r}
MSS = with(spruce.df, sum((height.qfit - mean(Height)) ^2 ))
MSS
```

## RSS
```{r}
RSS = with(spruce.df, sum((Height - height.qfit) ^2))
RSS
```

## MSS/TSS
```{r}
MSS/TSS
```

## Task 6
```{r}
cooks20x(quad.lm, main = "Cook's Distance plot for quad.lm")
```

## What is Cook's distance
Cook’s distance measures how much a single observation influences the overall regression model. It takes into account both the residual, which is how far the point is from the predicted value, and its leverage, which is how far the point is from the average of the predictors. A high Cook’s distance means that removing this point would noticeably change the model coefficients or predictions, while a low Cook’s distance means the point has little effect on the model. It helps identify outliers or influential points that can change the regression results.

## Cook's distance for quad.lm
In quad.lm there is a Cook's numebr that is 24 which means its the most influential. Its the largest number in the Cook's Plot

## quad2.lm
```{r}
quad2.lm = lm(Height~BHDiameter + I(BHDiameter ^ 2) , data = spruce.df[-24,])

summary(quad2.lm)
```

## Comparison with quad.lm
```{r}
summary(quad.lm)
```

## Conclusion
The Cook's plot was accurate in indicating that the 24th datum was impacting the model. When removed the r^2 value was increased meaning the model explained more of the variation in tree height. This suggests that the 24th point was pulling the regression line away from the overall trend. By excluding it, the model better represents the general relationship.

## Task 7

## Proof

If two lines share the point $x_k$:

\begin{align}
l_1 &: y = \beta_0 + \beta_1 x \\
l_2 &: y = \beta_0 + \delta + (\beta_1 + \beta_2)x
\end{align}

Plug in the point $x_k$ and set the equations equal:

\begin{align}
y_k &= \beta_0 + \beta_1 x_k \\
    &= \beta_0 + \delta + (\beta_1 + \beta_2)x_k
\end{align}

Distribute $x_k$ on the RHS:

\begin{align}
\beta_0 + \beta_1 x_k &= \beta_0 + \delta + \beta_1 x_k + \beta_2 x_k \\
0 &= \delta + \beta_2 x_k \\
\delta &= -\beta_2 x_k
\end{align}

Use $l_2$ to substitute $\delta = -\beta_2 x_k$:

\begin{align}
l_2 &: y = \beta_0 + \delta + (\beta_1 + \beta_2)x \\
l_2 &: y = \beta_0 - \beta_2 x_k + (\beta_1 + \beta_2)x \\
l_2 &: y = \beta_0 - \beta_2 x_k + \beta_1 x + \beta_2 x \\
l_2 &: y = \beta_0 + \beta_1 x + \beta_2(x - x_k)
\end{align}

Thus the piecewise regression form is:

\begin{align}
y = \beta_0 + \beta_1 x + \beta_2 (x - x_k) I(x \geq x_k)
\end{align}

where $I(\cdot) = 1$ if $x \geq x_k$ and $0$ otherwise.

## Reproducing plot
```{r}
sp2.df = within(spruce.df, x <- (BHDiameter - 18) * (BHDiameter > 18))

sp2.df
```

```{r}
lmp = lm(Height~BHDiameter + x, data = sp2.df)
tmp = summary(lmp)
names(tmp)
```

```{r}
myf = function(x, coef){
  coef[1] + coef[2] * (x) + coef[3] * (x-18) * (x-18>0)
}

```

```{r}
plot(spruce.df,main="Piecewise regression")
myf(0, coef=tmp$coefficients[,"Estimate"])
curve(myf(x,coef=tmp$coefficients[,"Estimate"] ),add=TRUE, lwd=2,col="Blue")
abline(v=18)
text(18,16,paste("R sq.=",round(tmp$r.squared,4) ))
```


## Task 8

The scatterhist() function creates a scatterplot of two numeric variables with marginal histograms. The scatterplot shows the relationship between x and y, while the histograms summarize the distributions of each variable along their respective axes.

```{r}
library(MATH4753F25robert)  

#random data to test if scatterhist works

set.seed(123)
x <- rnorm(100, mean = 50, sd = 10)
y <- rnorm(100, mean = 30, sd = 5)

MATH4753F25robert::scatterhist(x, y)

```














